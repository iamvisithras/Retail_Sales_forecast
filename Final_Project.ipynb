
#Packages
import pandas as pd
import warnings
warnings.filterwarnings("ignore")
import datetime as dt
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
Read the csv files
df_features=pd.read_csv("C:/Users/visithra/Desktop/New folder/Retail-Sales-Forecast/dataset/Features_data_set.csv")
df_features
Store	Date	Temperature	Fuel_Price	MarkDown1	MarkDown2	MarkDown3	MarkDown4	MarkDown5	CPI	Unemployment	IsHoliday
0	1	05/02/2010	42.31	2.572	NaN	NaN	NaN	NaN	NaN	211.096358	8.106	False
1	1	12/02/2010	38.51	2.548	NaN	NaN	NaN	NaN	NaN	211.242170	8.106	True
2	1	19/02/2010	39.93	2.514	NaN	NaN	NaN	NaN	NaN	211.289143	8.106	False
3	1	26/02/2010	46.63	2.561	NaN	NaN	NaN	NaN	NaN	211.319643	8.106	False
4	1	05/03/2010	46.50	2.625	NaN	NaN	NaN	NaN	NaN	211.350143	8.106	False
...	...	...	...	...	...	...	...	...	...	...	...	...
8185	45	28/06/2013	76.05	3.639	4842.29	975.03	3.00	2449.97	3169.69	NaN	NaN	False
8186	45	05/07/2013	77.50	3.614	9090.48	2268.58	582.74	5797.47	1514.93	NaN	NaN	False
8187	45	12/07/2013	79.37	3.614	3789.94	1827.31	85.72	744.84	2150.36	NaN	NaN	False
8188	45	19/07/2013	82.84	3.737	2961.49	1047.07	204.19	363.00	1059.46	NaN	NaN	False
8189	45	26/07/2013	76.06	3.804	212.02	851.73	2.06	10.88	1864.57	NaN	NaN	False
8190 rows × 12 columns

df_stores=pd.read_csv("C:/Users/visithra/Desktop/New folder/Retail-Sales-Forecast/dataset/stores_data_set.csv")
df_stores
Store	Type	Size
0	1	A	151315
1	2	A	202307
2	3	B	37392
3	4	A	205863
4	5	B	34875
5	6	A	202505
6	7	B	70713
7	8	A	155078
8	9	B	125833
9	10	B	126512
10	11	A	207499
11	12	B	112238
12	13	A	219622
13	14	A	200898
14	15	B	123737
15	16	B	57197
16	17	B	93188
17	18	B	120653
18	19	A	203819
19	20	A	203742
20	21	B	140167
21	22	B	119557
22	23	B	114533
23	24	A	203819
24	25	B	128107
25	26	A	152513
26	27	A	204184
27	28	A	206302
28	29	B	93638
29	30	C	42988
30	31	A	203750
31	32	A	203007
32	33	A	39690
33	34	A	158114
34	35	B	103681
35	36	A	39910
36	37	C	39910
37	38	C	39690
38	39	A	184109
39	40	A	155083
40	41	A	196321
41	42	C	39690
42	43	C	41062
43	44	C	39910
44	45	B	118221
df_sales= pd.read_csv("C:/Users/visithra/Desktop/New folder/Retail-Sales-Forecast/dataset/sales_data_set.csv")
df_sales
Store	Dept	Date	Weekly_Sales	IsHoliday
0	1	1	05/02/2010	24924.50	False
1	1	1	12/02/2010	46039.49	True
2	1	1	19/02/2010	41595.55	False
3	1	1	26/02/2010	19403.54	False
4	1	1	05/03/2010	21827.90	False
...	...	...	...	...	...
421565	45	98	28/09/2012	508.37	False
421566	45	98	05/10/2012	628.10	False
421567	45	98	12/10/2012	1061.02	False
421568	45	98	19/10/2012	760.01	False
421569	45	98	26/10/2012	1076.80	False
421570 rows × 5 columns

Mergeing the store data
#Features and stores
df1=pd.merge(df_features,df_stores,on="Store",how="inner")
df1
Store	Date	Temperature	Fuel_Price	MarkDown1	MarkDown2	MarkDown3	MarkDown4	MarkDown5	CPI	Unemployment	IsHoliday	Type	Size
0	1	05/02/2010	42.31	2.572	NaN	NaN	NaN	NaN	NaN	211.096358	8.106	False	A	151315
1	1	12/02/2010	38.51	2.548	NaN	NaN	NaN	NaN	NaN	211.242170	8.106	True	A	151315
2	1	19/02/2010	39.93	2.514	NaN	NaN	NaN	NaN	NaN	211.289143	8.106	False	A	151315
3	1	26/02/2010	46.63	2.561	NaN	NaN	NaN	NaN	NaN	211.319643	8.106	False	A	151315
4	1	05/03/2010	46.50	2.625	NaN	NaN	NaN	NaN	NaN	211.350143	8.106	False	A	151315
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
8185	45	28/06/2013	76.05	3.639	4842.29	975.03	3.00	2449.97	3169.69	NaN	NaN	False	B	118221
8186	45	05/07/2013	77.50	3.614	9090.48	2268.58	582.74	5797.47	1514.93	NaN	NaN	False	B	118221
8187	45	12/07/2013	79.37	3.614	3789.94	1827.31	85.72	744.84	2150.36	NaN	NaN	False	B	118221
8188	45	19/07/2013	82.84	3.737	2961.49	1047.07	204.19	363.00	1059.46	NaN	NaN	False	B	118221
8189	45	26/07/2013	76.06	3.804	212.02	851.73	2.06	10.88	1864.57	NaN	NaN	False	B	118221
8190 rows × 14 columns

#Sales and stores
df2=pd.merge(df_sales,df_stores,on="Store",how="inner")
df2
Store	Dept	Date	Weekly_Sales	IsHoliday	Type	Size
0	1	1	05/02/2010	24924.50	False	A	151315
1	1	1	12/02/2010	46039.49	True	A	151315
2	1	1	19/02/2010	41595.55	False	A	151315
3	1	1	26/02/2010	19403.54	False	A	151315
4	1	1	05/03/2010	21827.90	False	A	151315
...	...	...	...	...	...	...	...
421565	45	98	28/09/2012	508.37	False	B	118221
421566	45	98	05/10/2012	628.10	False	B	118221
421567	45	98	12/10/2012	1061.02	False	B	118221
421568	45	98	19/10/2012	760.01	False	B	118221
421569	45	98	26/10/2012	1076.80	False	B	118221
421570 rows × 7 columns

#df1 have (2010 to 2013) and df2 have (2010 to 2012)
#so we need to splite the df1's (2010 to 2012)
spliting and merging
#creating the commen columns for merging
df1["differ"]=df1["Store"].astype(str)+"-"+df1["Date"]
df2["differ"]=df2["Store"].astype(str)+"-"+df2["Date"]
df1_inlist= df1[df1["differ"].isin(df2["differ"])]
df1_inlist.reset_index(drop=True,inplace=True)
df1_inlist.tail(3)
Store	Date	Temperature	Fuel_Price	MarkDown1	MarkDown2	MarkDown3	MarkDown4	MarkDown5	CPI	Unemployment	IsHoliday	Type	Size	differ
6432	45	12/10/2012	54.47	4.000	1956.28	NaN	7.89	599.32	3990.54	192.327265	8.667	False	B	118221	45-12/10/2012
6433	45	19/10/2012	56.47	3.969	2004.02	NaN	3.18	437.73	1537.49	192.330854	8.667	False	B	118221	45-19/10/2012
6434	45	26/10/2012	58.85	3.882	4018.91	58.08	100.00	211.94	858.33	192.308899	8.667	False	B	118221	45-26/10/2012
df1_notinlist=df1[~df1["differ"].isin(df2["differ"])]
df1_notinlist.reset_index(drop=True,inplace=True)
df1_notinlist.tail(3)
Store	Date	Temperature	Fuel_Price	MarkDown1	MarkDown2	MarkDown3	MarkDown4	MarkDown5	CPI	Unemployment	IsHoliday	Type	Size	differ
1752	45	12/07/2013	79.37	3.614	3789.94	1827.31	85.72	744.84	2150.36	NaN	NaN	False	B	118221	45-12/07/2013
1753	45	19/07/2013	82.84	3.737	2961.49	1047.07	204.19	363.00	1059.46	NaN	NaN	False	B	118221	45-19/07/2013
1754	45	26/07/2013	76.06	3.804	212.02	851.73	2.06	10.88	1864.57	NaN	NaN	False	B	118221	45-26/07/2013
#checking the shape and the unique values
df1.shape,df1_inlist.shape,df1_notinlist.shape
((8190, 15), (6435, 15), (1755, 15))
#Total rows in df1   ==> 8190
#2010 to 2012 in df1 ==> 6435
#2012 to 2013 in df1 ==> 1755
df1["differ"].nunique(),df1_inlist["differ"].nunique(),df1_notinlist["differ"].nunique()
#so we don't have any duplicate values in the df1
(8190, 6435, 1755)
df3=pd.merge(df2,df1_inlist,on="differ",how="inner")
df3.columns
Index(['Store_x', 'Dept', 'Date_x', 'Weekly_Sales', 'IsHoliday_x', 'Type_x',
       'Size_x', 'differ', 'Store_y', 'Date_y', 'Temperature', 'Fuel_Price',
       'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI',
       'Unemployment', 'IsHoliday_y', 'Type_y', 'Size_y'],
      dtype='object')
#droping the dublicate columns

df3.drop(columns=['Store_y','Date_y','IsHoliday_y','Type_y','Size_y'],inplace=True)
#rename the columns
df3.rename(columns={"Store_x":"Store","Date_x":"Date","IsHoliday_x":"IsHoliday","Type_x":"Type","Size_x":"Size"},inplace=True)
# Now we want to  merge the dataframe df3 and df1_notinlist(2012 - 2013) row wise
# But df3 have a "Dept" and df1_notinlist have not "Dept"
# So we want to add the "Dept" column in df1_notinlist
# Take the "Dept" column from sales Dataset

sd=df_sales[["Store","Dept"]]
sd
Store	Dept
0	1	1
1	1	1
2	1	1
3	1	1
4	1	1
...	...	...
421565	45	98
421566	45	98
421567	45	98
421568	45	98
421569	45	98
421570 rows × 2 columns

sd.drop_duplicates(subset=["Store","Dept"],inplace=True)
sd.reset_index(drop=True, inplace=True)
sd.tail(3)
Store	Dept
3328	45	96
3329	45	97
3330	45	98
df1_notinlist
Store	Date	Temperature	Fuel_Price	MarkDown1	MarkDown2	MarkDown3	MarkDown4	MarkDown5	CPI	Unemployment	IsHoliday	Type	Size	differ
0	1	02/11/2012	55.32	3.386	6766.44	5147.70	50.82	3639.90	2737.42	223.462779	6.573	False	A	151315	1-02/11/2012
1	1	09/11/2012	61.24	3.314	11421.32	3370.89	40.28	4646.79	6154.16	223.481307	6.573	False	A	151315	1-09/11/2012
2	1	16/11/2012	52.92	3.252	9696.28	292.10	103.78	1133.15	6612.69	223.512911	6.573	False	A	151315	1-16/11/2012
3	1	23/11/2012	56.23	3.211	883.59	4.17	74910.32	209.91	303.32	223.561947	6.573	True	A	151315	1-23/11/2012
4	1	30/11/2012	52.34	3.207	2460.03	NaN	3838.35	150.57	6966.34	223.610984	6.573	False	A	151315	1-30/11/2012
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
1750	45	28/06/2013	76.05	3.639	4842.29	975.03	3.00	2449.97	3169.69	NaN	NaN	False	B	118221	45-28/06/2013
1751	45	05/07/2013	77.50	3.614	9090.48	2268.58	582.74	5797.47	1514.93	NaN	NaN	False	B	118221	45-05/07/2013
1752	45	12/07/2013	79.37	3.614	3789.94	1827.31	85.72	744.84	2150.36	NaN	NaN	False	B	118221	45-12/07/2013
1753	45	19/07/2013	82.84	3.737	2961.49	1047.07	204.19	363.00	1059.46	NaN	NaN	False	B	118221	45-19/07/2013
1754	45	26/07/2013	76.06	3.804	212.02	851.73	2.06	10.88	1864.57	NaN	NaN	False	B	118221	45-26/07/2013
1755 rows × 15 columns

df4= pd.merge(sd,df1_notinlist,on="Store",how="outer")
df4
Store	Dept	Date	Temperature	Fuel_Price	MarkDown1	MarkDown2	MarkDown3	MarkDown4	MarkDown5	CPI	Unemployment	IsHoliday	Type	Size	differ
0	1	1	02/11/2012	55.32	3.386	6766.44	5147.70	50.82	3639.90	2737.42	223.462779	6.573	False	A	151315	1-02/11/2012
1	1	1	09/11/2012	61.24	3.314	11421.32	3370.89	40.28	4646.79	6154.16	223.481307	6.573	False	A	151315	1-09/11/2012
2	1	1	16/11/2012	52.92	3.252	9696.28	292.10	103.78	1133.15	6612.69	223.512911	6.573	False	A	151315	1-16/11/2012
3	1	1	23/11/2012	56.23	3.211	883.59	4.17	74910.32	209.91	303.32	223.561947	6.573	True	A	151315	1-23/11/2012
4	1	1	30/11/2012	52.34	3.207	2460.03	NaN	3838.35	150.57	6966.34	223.610984	6.573	False	A	151315	1-30/11/2012
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
129904	45	98	28/06/2013	76.05	3.639	4842.29	975.03	3.00	2449.97	3169.69	NaN	NaN	False	B	118221	45-28/06/2013
129905	45	98	05/07/2013	77.50	3.614	9090.48	2268.58	582.74	5797.47	1514.93	NaN	NaN	False	B	118221	45-05/07/2013
129906	45	98	12/07/2013	79.37	3.614	3789.94	1827.31	85.72	744.84	2150.36	NaN	NaN	False	B	118221	45-12/07/2013
129907	45	98	19/07/2013	82.84	3.737	2961.49	1047.07	204.19	363.00	1059.46	NaN	NaN	False	B	118221	45-19/07/2013
129908	45	98	26/07/2013	76.06	3.804	212.02	851.73	2.06	10.88	1864.57	NaN	NaN	False	B	118221	45-26/07/2013
129909 rows × 16 columns

df5=pd.concat([df3,df4])
df5.reset_index(drop=True,inplace=True)
df5
Store	Dept	Date	Weekly_Sales	IsHoliday	Type	Size	differ	Temperature	Fuel_Price	MarkDown1	MarkDown2	MarkDown3	MarkDown4	MarkDown5	CPI	Unemployment
0	1	1	05/02/2010	24924.50	False	A	151315	1-05/02/2010	42.31	2.572	NaN	NaN	NaN	NaN	NaN	211.096358	8.106
1	1	2	05/02/2010	50605.27	False	A	151315	1-05/02/2010	42.31	2.572	NaN	NaN	NaN	NaN	NaN	211.096358	8.106
2	1	3	05/02/2010	13740.12	False	A	151315	1-05/02/2010	42.31	2.572	NaN	NaN	NaN	NaN	NaN	211.096358	8.106
3	1	4	05/02/2010	39954.04	False	A	151315	1-05/02/2010	42.31	2.572	NaN	NaN	NaN	NaN	NaN	211.096358	8.106
4	1	5	05/02/2010	32229.38	False	A	151315	1-05/02/2010	42.31	2.572	NaN	NaN	NaN	NaN	NaN	211.096358	8.106
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
551474	45	98	28/06/2013	NaN	False	B	118221	45-28/06/2013	76.05	3.639	4842.29	975.03	3.00	2449.97	3169.69	NaN	NaN
551475	45	98	05/07/2013	NaN	False	B	118221	45-05/07/2013	77.50	3.614	9090.48	2268.58	582.74	5797.47	1514.93	NaN	NaN
551476	45	98	12/07/2013	NaN	False	B	118221	45-12/07/2013	79.37	3.614	3789.94	1827.31	85.72	744.84	2150.36	NaN	NaN
551477	45	98	19/07/2013	NaN	False	B	118221	45-19/07/2013	82.84	3.737	2961.49	1047.07	204.19	363.00	1059.46	NaN	NaN
551478	45	98	26/07/2013	NaN	False	B	118221	45-26/07/2013	76.06	3.804	212.02	851.73	2.06	10.88	1864.57	NaN	NaN
551479 rows × 17 columns

df5["Date"]=df5["Date"].apply(lambda x: x.replace("/","-"))
df5["Date"]
0         05-02-2010
1         05-02-2010
2         05-02-2010
3         05-02-2010
4         05-02-2010
             ...    
551474    28-06-2013
551475    05-07-2013
551476    12-07-2013
551477    19-07-2013
551478    26-07-2013
Name: Date, Length: 551479, dtype: object
df5["Date"]= pd.to_datetime(df5["Date"],format="%d-%m-%Y",errors="coerce")
df5["Date"]
0        2010-02-05
1        2010-02-05
2        2010-02-05
3        2010-02-05
4        2010-02-05
            ...    
551474   2013-06-28
551475   2013-07-05
551476   2013-07-12
551477   2013-07-19
551478   2013-07-26
Name: Date, Length: 551479, dtype: datetime64[ns]
df5["IsHoliday"]= df5["IsHoliday"].map({False:0, True:1})
df5["Type"]= df5["Type"].map({"A":0,"B":1,"C":2})
df5
Store	Dept	Date	Weekly_Sales	IsHoliday	Type	Size	differ	Temperature	Fuel_Price	MarkDown1	MarkDown2	MarkDown3	MarkDown4	MarkDown5	CPI	Unemployment
0	1	1	2010-02-05	24924.50	0	0	151315	1-05/02/2010	42.31	2.572	NaN	NaN	NaN	NaN	NaN	211.096358	8.106
1	1	2	2010-02-05	50605.27	0	0	151315	1-05/02/2010	42.31	2.572	NaN	NaN	NaN	NaN	NaN	211.096358	8.106
2	1	3	2010-02-05	13740.12	0	0	151315	1-05/02/2010	42.31	2.572	NaN	NaN	NaN	NaN	NaN	211.096358	8.106
3	1	4	2010-02-05	39954.04	0	0	151315	1-05/02/2010	42.31	2.572	NaN	NaN	NaN	NaN	NaN	211.096358	8.106
4	1	5	2010-02-05	32229.38	0	0	151315	1-05/02/2010	42.31	2.572	NaN	NaN	NaN	NaN	NaN	211.096358	8.106
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
551474	45	98	2013-06-28	NaN	0	1	118221	45-28/06/2013	76.05	3.639	4842.29	975.03	3.00	2449.97	3169.69	NaN	NaN
551475	45	98	2013-07-05	NaN	0	1	118221	45-05/07/2013	77.50	3.614	9090.48	2268.58	582.74	5797.47	1514.93	NaN	NaN
551476	45	98	2013-07-12	NaN	0	1	118221	45-12/07/2013	79.37	3.614	3789.94	1827.31	85.72	744.84	2150.36	NaN	NaN
551477	45	98	2013-07-19	NaN	0	1	118221	45-19/07/2013	82.84	3.737	2961.49	1047.07	204.19	363.00	1059.46	NaN	NaN
551478	45	98	2013-07-26	NaN	0	1	118221	45-26/07/2013	76.06	3.804	212.02	851.73	2.06	10.88	1864.57	NaN	NaN
551479 rows × 17 columns

df5.drop(columns=["differ"],inplace=True)
df5
Store	Dept	Date	Weekly_Sales	IsHoliday	Type	Size	Temperature	Fuel_Price	MarkDown1	MarkDown2	MarkDown3	MarkDown4	MarkDown5	CPI	Unemployment
0	1	1	2010-02-05	24924.50	0	0	151315	42.31	2.572	NaN	NaN	NaN	NaN	NaN	211.096358	8.106
1	1	2	2010-02-05	50605.27	0	0	151315	42.31	2.572	NaN	NaN	NaN	NaN	NaN	211.096358	8.106
2	1	3	2010-02-05	13740.12	0	0	151315	42.31	2.572	NaN	NaN	NaN	NaN	NaN	211.096358	8.106
3	1	4	2010-02-05	39954.04	0	0	151315	42.31	2.572	NaN	NaN	NaN	NaN	NaN	211.096358	8.106
4	1	5	2010-02-05	32229.38	0	0	151315	42.31	2.572	NaN	NaN	NaN	NaN	NaN	211.096358	8.106
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
551474	45	98	2013-06-28	NaN	0	1	118221	76.05	3.639	4842.29	975.03	3.00	2449.97	3169.69	NaN	NaN
551475	45	98	2013-07-05	NaN	0	1	118221	77.50	3.614	9090.48	2268.58	582.74	5797.47	1514.93	NaN	NaN
551476	45	98	2013-07-12	NaN	0	1	118221	79.37	3.614	3789.94	1827.31	85.72	744.84	2150.36	NaN	NaN
551477	45	98	2013-07-19	NaN	0	1	118221	82.84	3.737	2961.49	1047.07	204.19	363.00	1059.46	NaN	NaN
551478	45	98	2013-07-26	NaN	0	1	118221	76.06	3.804	212.02	851.73	2.06	10.88	1864.57	NaN	NaN
551479 rows × 16 columns

df5=df5.sort_values(by=["Date","Store","Dept"])
df5.reset_index(drop=True,inplace=True)
df5["Day"]=df5["Date"].dt.day
df5["Month"]=df5["Date"].dt.month
df5["Year"]=df5["Date"].dt.year
df5.drop(columns=["Date"],inplace=True)
df5.columns
Index(['Store', 'Dept', 'Weekly_Sales', 'IsHoliday', 'Type', 'Size',
       'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3',
       'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Day', 'Month',
       'Year'],
      dtype='object')
df5=df5[['Day','Month','Year','Store','Dept','Type','Weekly_Sales','Size','IsHoliday','Temperature',
           'Fuel_Price', 'MarkDown1', 'MarkDown2','MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment']]
df5.describe()
Day	Month	Year	Store	Dept	Type	Weekly_Sales	Size	IsHoliday	Temperature	Fuel_Price	MarkDown1	MarkDown2	MarkDown3	MarkDown4	MarkDown5	CPI	Unemployment
count	551479.000000	551479.000000	551479.000000	551479.000000	551479.000000	551479.000000	421570.000000	551479.000000	551479.000000	551479.000000	551479.000000	280405.000000	207897.000000	254619.000000	248649.000000	281341.000000	508176.000000	508176.000000
mean	15.653488	6.277180	2011.392758	22.270485	44.603013	0.594791	15981.258123	136073.691863	0.071905	58.703624	3.413010	7369.403469	3489.575965	1859.728572	3355.891061	4261.631495	172.200115	7.775470
std	8.776044	3.340134	1.054086	12.796872	30.416079	0.669624	22711.183519	61207.331445	0.258330	18.685408	0.427725	9439.253385	8920.862286	11633.662079	6889.409946	13679.851274	39.595683	1.863253
min	1.000000	1.000000	2010.000000	1.000000	1.000000	0.000000	-4988.940000	34875.000000	0.000000	-7.290000	2.472000	-2781.450000	-265.760000	-179.260000	0.220000	-185.170000	126.064000	3.684000
25%	8.000000	3.000000	2010.000000	11.000000	19.000000	0.000000	2079.650000	93638.000000	0.000000	45.160000	3.050000	2014.470000	74.240000	7.330000	319.190000	1524.870000	132.529129	6.614000
50%	16.000000	6.000000	2011.000000	22.000000	38.000000	0.000000	7612.030000	128107.000000	0.000000	59.940000	3.524000	5011.720000	397.960000	41.390000	1202.440000	2825.360000	182.517732	7.771000
75%	23.000000	9.000000	2012.000000	33.000000	74.000000	1.000000	20205.852500	202505.000000	0.000000	73.160000	3.746000	9264.480000	2359.440000	177.270000	3380.280000	4930.020000	213.871114	8.549000
max	31.000000	12.000000	2013.000000	45.000000	99.000000	2.000000	693099.360000	219622.000000	1.000000	101.950000	4.468000	103184.980000	104519.540000	149483.310000	67474.850000	771448.100000	228.976456	14.313000
# Weekly sales have a negative values
# so we want to change the negative values into null
df5[df5["Weekly_Sales"]<=0]
Day	Month	Year	Store	Dept	Type	Weekly_Sales	Size	IsHoliday	Temperature	Fuel_Price	MarkDown1	MarkDown2	MarkDown3	MarkDown4	MarkDown5	CPI	Unemployment
389	5	2	2010	6	47	0	-59.00	202505	0	40.43	2.572	NaN	NaN	NaN	NaN	NaN	212.622352	7.259
601	5	2	2010	9	49	1	-15.00	125833	0	38.01	2.572	NaN	NaN	NaN	NaN	NaN	214.655459	6.415
714	5	2	2010	11	19	0	0.00	207499	0	46.04	2.572	NaN	NaN	NaN	NaN	NaN	214.424881	7.368
1039	5	2	2010	15	80	1	-0.04	123737	0	19.83	2.954	NaN	NaN	NaN	NaN	NaN	131.527903	8.350
1345	5	2	2010	20	19	0	0.00	203742	0	25.92	2.784	NaN	NaN	NaN	NaN	NaN	204.247194	8.187
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
419896	26	10	2012	19	47	0	-18.00	203819	0	56.49	4.071	5430.75	90.07	NaN	904.34	1665.77	138.728161	7.992
419943	26	10	2012	20	19	0	-3.97	203742	0	60.04	3.882	10813.19	22.33	102.91	867.39	1671.76	216.151590	7.293
420601	26	10	2012	29	80	1	-178.15	93638	0	57.58	3.917	5581.80	2.98	NaN	398.23	396.11	138.728161	9.151
420972	26	10	2012	35	80	1	-16.51	103681	0	58.99	3.882	6221.06	42.48	103.00	273.90	996.79	142.762411	8.665
421375	26	10	2012	42	44	2	-2.47	39690	0	70.50	4.301	NaN	0.75	2.00	NaN	1034.55	131.193097	6.943
1358 rows × 18 columns

df5["Weekly_Sales"]=df5["Weekly_Sales"].apply(lambda x: np.nan if x<=0 else x)
df5.isnull().sum()
Day                  0
Month                0
Year                 0
Store                0
Dept                 0
Type                 0
Weekly_Sales    131267
Size                 0
IsHoliday            0
Temperature          0
Fuel_Price           0
MarkDown1       271074
MarkDown2       343582
MarkDown3       296860
MarkDown4       302830
MarkDown5       270138
CPI              43303
Unemployment     43303
dtype: int64
df5["Markdown"]=df5[["MarkDown1","MarkDown2","MarkDown3","MarkDown4","MarkDown5"]].notnull().any(axis=1)
df5["Markdown"]=df5["Markdown"].map({True:1,False:0})
df5.groupby("Markdown")["Weekly_Sales"].mean()
Markdown
0    15922.137810
1    16231.116041
Name: Weekly_Sales, dtype: float64
df5_corr=df5.corr()
plt.figure(figsize=(15,8))
sns.heatmap(df5_corr,annot=True,cmap="Blues",fmt=".2f")
plt.show()

****Removed Null Values Correlation****#
df5_dropna=df5.dropna()
df5_dropna_corr=df5_dropna.corr()
plt.figure(figsize=(15,8))
sns.heatmap(df5_dropna_corr,annot=True,cmap="Blues",fmt=".2f")
plt.show()

#Weekly salse have higly correlation with the "Size","Type","Dept"
#Other wise have low correlation
#creating a "differ" columns for the usage of merging other dataframes
df5["differ"]=df5["Day"].astype(str)+df5["Month"].astype(str)+df5["Year"].astype(str)+"-"+df5["Store"].astype(str)+"-"+df5["Dept"].astype(str)
df5.isnull().sum()
Day                  0
Month                0
Year                 0
Store                0
Dept                 0
Type                 0
Weekly_Sales    131267
Size                 0
IsHoliday            0
Temperature          0
Fuel_Price           0
MarkDown1       271074
MarkDown2       343582
MarkDown3       296860
MarkDown4       302830
MarkDown5       270138
CPI              43303
Unemployment     43303
Markdown             0
differ               0
dtype: int64
#MarkDown have a more null values then other columns
df5.nunique()
Day                 31
Month               12
Year                 4
Store               45
Dept                81
Type                 3
Weekly_Sales    358785
Size                40
IsHoliday            2
Temperature       4178
Fuel_Price        1011
MarkDown1         4023
MarkDown2         2715
MarkDown3         2885
MarkDown4         3405
MarkDown5         4045
CPI               2505
Unemployment       404
Markdown             2
differ          551479
dtype: int64
#importing the ML packages for the model selection

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
import pickle
df5
Day	Month	Year	Store	Dept	Type	Weekly_Sales	Size	IsHoliday	Temperature	Fuel_Price	MarkDown1	MarkDown2	MarkDown3	MarkDown4	MarkDown5	CPI	Unemployment	Markdown	differ
0	5	2	2010	1	1	0	24924.50	151315	0	42.31	2.572	NaN	NaN	NaN	NaN	NaN	211.096358	8.106	0	522010-1-1
1	5	2	2010	1	2	0	50605.27	151315	0	42.31	2.572	NaN	NaN	NaN	NaN	NaN	211.096358	8.106	0	522010-1-2
2	5	2	2010	1	3	0	13740.12	151315	0	42.31	2.572	NaN	NaN	NaN	NaN	NaN	211.096358	8.106	0	522010-1-3
3	5	2	2010	1	4	0	39954.04	151315	0	42.31	2.572	NaN	NaN	NaN	NaN	NaN	211.096358	8.106	0	522010-1-4
4	5	2	2010	1	5	0	32229.38	151315	0	42.31	2.572	NaN	NaN	NaN	NaN	NaN	211.096358	8.106	0	522010-1-5
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
551474	26	7	2013	45	94	1	NaN	118221	0	76.06	3.804	212.02	851.73	2.06	10.88	1864.57	NaN	NaN	1	2672013-45-94
551475	26	7	2013	45	95	1	NaN	118221	0	76.06	3.804	212.02	851.73	2.06	10.88	1864.57	NaN	NaN	1	2672013-45-95
551476	26	7	2013	45	96	1	NaN	118221	0	76.06	3.804	212.02	851.73	2.06	10.88	1864.57	NaN	NaN	1	2672013-45-96
551477	26	7	2013	45	97	1	NaN	118221	0	76.06	3.804	212.02	851.73	2.06	10.88	1864.57	NaN	NaN	1	2672013-45-97
551478	26	7	2013	45	98	1	NaN	118221	0	76.06	3.804	212.02	851.73	2.06	10.88	1864.57	NaN	NaN	1	2672013-45-98
551479 rows × 20 columns

df5_m1=df5.copy()
df5_m1.drop(columns=["Markdown"],inplace=True)
df5_m1.isnull().sum()
Day                  0
Month                0
Year                 0
Store                0
Dept                 0
Type                 0
Weekly_Sales    131267
Size                 0
IsHoliday            0
Temperature          0
Fuel_Price           0
MarkDown1       271074
MarkDown2       343582
MarkDown3       296860
MarkDown4       302830
MarkDown5       270138
CPI              43303
Unemployment     43303
differ               0
dtype: int64
#function for the checking the accurcy score(r2_score)
def accuracy_score(x_train,x_test,y_train,y_test,algorithm):
    model=algorithm().fit(x_train,y_train)
    y_pred_train= model.predict(x_train)
    y_pred_test= model.predict(x_test)

    r2_score_train=r2_score(y_pred_train,y_train)
    r2_score_test= r2_score(y_pred_test,y_test)

    accuracy={"algorithm":algorithm,
              "R2_train":r2_score_train,
              "R2_test":r2_score_test}
    
    return accuracy
#chooshing the best model

def ml_regressor(df,null_columns,lable):
    #droping the null columns
    df1=df.drop(columns=null_columns)

    #separate the null columns for lable
    df1_null=df1[df1[lable].isnull()]
    df1_null.reset_index(drop=True,inplace=True)

    #separate the notnull columns for lable
    df1_notnull= df1[df1[lable].notnull()]
    df1_notnull.reset_index(drop=True,inplace=True)

    # chooshig the x and y value for the ML using notnull dataframe
    x=df1_notnull.drop(columns=[lable,"differ"])
    y=df1_notnull[lable]

    #train_test_spliting
    x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)

    #checking the ML models
    print(accuracy_score(x_train,x_test,y_train,y_test,DecisionTreeRegressor))
    print(accuracy_score(x_train,x_test,y_train,y_test,RandomForestRegressor))
    print(accuracy_score(x_train,x_test,y_train,y_test,AdaBoostRegressor))
    print(accuracy_score(x_train,x_test,y_train,y_test,ExtraTreesRegressor))
    print(accuracy_score(x_train,x_test,y_train,y_test,GradientBoostingRegressor))
    print(accuracy_score(x_train,x_test,y_train,y_test,XGBRegressor))
    
# ML function for RandomForestRegresser
def RandomForest(df,null_columns,lable):
    #droping the null columns
    df1=df.drop(columns=null_columns)

    #separate the null columns for lable
    df1_null=df1[df1[lable].isnull()]
    df1_null.reset_index(drop=True,inplace=True)

    #separate the notnull columns for lable
    df1_notnull= df1[df1[lable].notnull()]
    df1_notnull.reset_index(drop=True,inplace=True)

    # chooshig the x and y value for the ML using notnull dataframe
    x=df1_notnull.drop(columns=[lable,"differ"])
    y=df1_notnull[lable]

    #train_test_spliting
    x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)

    model=RandomForestRegressor().fit(x_train,y_train)
    y_pred= model.predict(x_test)

    #evaluatet the model
    mse=mean_squared_error(y_pred,y_test)
    rmse= np.sqrt(mse)
    mae=mean_absolute_error(y_pred,y_test)
    r2= r2_score(y_pred,y_test)

    metrics={"Mean_squared_error":mse,
             "Root_mean_squared_error":rmse,
             "Mean_absolute_error":mae,
             "R2_score":r2}
    
    print(metrics)
    
    #predict the null values for the df1_null dataframe
    test_data=df1_null.drop(columns=[lable,"differ"],axis=1)
    y_pred_null_df=model.predict(test_data)

    df1_null[lable]= pd.DataFrame(y_pred_null_df)

    df_result= pd.concat([df1_null,df1_notnull], axis=0, ignore_index=True)

    return df_result
df5_m1.columns
Index(['Day', 'Month', 'Year', 'Store', 'Dept', 'Type', 'Weekly_Sales', 'Size',
       'IsHoliday', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2',
       'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'differ'],
      dtype='object')
df5_m1_md1=RandomForest(df5_m1,['Weekly_Sales','MarkDown2','MarkDown3','MarkDown4','MarkDown5','CPI','Unemployment'],'MarkDown1')
{'Mean_squared_error': 3.102522892191884, 'Root_mean_squared_error': 1.7613979936947481, 'Mean_absolute_error': 0.028266885407147783, 'R2_score': 0.999999965959627}
df5_m1_md2=RandomForest(df5_m1,['Weekly_Sales','MarkDown1','MarkDown3','MarkDown4','MarkDown5','CPI','Unemployment'],'MarkDown2')
{'Mean_squared_error': 0.0003172398249158187, 'Root_mean_squared_error': 0.017811227496043577, 'Mean_absolute_error': 0.00021921116322873578, 'R2_score': 0.9999999999959083}
df5_m1_md3=RandomForest(df5_m1,['Weekly_Sales','MarkDown1','MarkDown2','MarkDown4','MarkDown5','CPI','Unemployment'],'MarkDown3')
{'Mean_squared_error': 0.0003261104119864575, 'Root_mean_squared_error': 0.018058527403596824, 'Mean_absolute_error': 0.0004140955171521932, 'R2_score': 0.9999999999976386}
df5_m1_md4=RandomForest(df5_m1,['Weekly_Sales','MarkDown1','MarkDown2','MarkDown3','MarkDown5','CPI','Unemployment'],'MarkDown4')
{'Mean_squared_error': 0.00025067410255379065, 'Root_mean_squared_error': 0.015832690944807538, 'Mean_absolute_error': 0.00010040619754557427, 'R2_score': 0.9999999999948828}
df5_m1_md5=RandomForest(df5_m1,['Weekly_Sales','MarkDown1','MarkDown2','MarkDown3','MarkDown4','CPI','Unemployment'],'MarkDown5')
{'Mean_squared_error': 7.189369816205203e-23, 'Root_mean_squared_error': 8.479015164631564e-12, 'Mean_absolute_error': 4.868333010077398e-12, 'R2_score': 1.0}
df5_m1_MD=df5_m1.drop(columns=["MarkDown1","MarkDown2","MarkDown3","MarkDown4","MarkDown5"],axis=1)
df5_m1_md1
Day	Month	Year	Store	Dept	Type	Size	IsHoliday	Temperature	Fuel_Price	MarkDown1	differ
0	5	2	2010	1	1	0	151315	0	42.31	2.572	6707.7404	522010-1-1
1	5	2	2010	1	2	0	151315	0	42.31	2.572	6707.7404	522010-1-2
2	5	2	2010	1	3	0	151315	0	42.31	2.572	6707.7404	522010-1-3
3	5	2	2010	1	4	0	151315	0	42.31	2.572	6707.7404	522010-1-4
4	5	2	2010	1	5	0	151315	0	42.31	2.572	6707.7404	522010-1-5
...	...	...	...	...	...	...	...	...	...	...	...	...
551474	26	7	2013	45	94	1	118221	0	76.06	3.804	212.0200	2672013-45-94
551475	26	7	2013	45	95	1	118221	0	76.06	3.804	212.0200	2672013-45-95
551476	26	7	2013	45	96	1	118221	0	76.06	3.804	212.0200	2672013-45-96
551477	26	7	2013	45	97	1	118221	0	76.06	3.804	212.0200	2672013-45-97
551478	26	7	2013	45	98	1	118221	0	76.06	3.804	212.0200	2672013-45-98
551479 rows × 12 columns

df5_m1_md5
Day	Month	Year	Store	Dept	Type	Size	IsHoliday	Temperature	Fuel_Price	MarkDown5	differ
0	5	2	2010	1	1	0	151315	0	42.31	2.572	4855.9997	522010-1-1
1	5	2	2010	1	2	0	151315	0	42.31	2.572	4855.9997	522010-1-2
2	5	2	2010	1	3	0	151315	0	42.31	2.572	4855.9997	522010-1-3
3	5	2	2010	1	4	0	151315	0	42.31	2.572	4855.9997	522010-1-4
4	5	2	2010	1	5	0	151315	0	42.31	2.572	4855.9997	522010-1-5
...	...	...	...	...	...	...	...	...	...	...	...	...
551474	26	7	2013	45	94	1	118221	0	76.06	3.804	1864.5700	2672013-45-94
551475	26	7	2013	45	95	1	118221	0	76.06	3.804	1864.5700	2672013-45-95
551476	26	7	2013	45	96	1	118221	0	76.06	3.804	1864.5700	2672013-45-96
551477	26	7	2013	45	97	1	118221	0	76.06	3.804	1864.5700	2672013-45-97
551478	26	7	2013	45	98	1	118221	0	76.06	3.804	1864.5700	2672013-45-98
551479 rows × 12 columns

df5_m1_MD=pd.merge(df5_m1_MD,df5_m1_md1[["MarkDown1","differ"]],on="differ",how="inner")
df5_m1_MD=pd.merge(df5_m1_MD,df5_m1_md2[["MarkDown2","differ"]],on="differ",how="inner")
df5_m1_MD=pd.merge(df5_m1_MD,df5_m1_md3[["MarkDown3","differ"]],on="differ",how="inner")
df5_m1_MD=pd.merge(df5_m1_MD,df5_m1_md4[["MarkDown4","differ"]],on="differ",how="inner")
df5_m1_MD=pd.merge(df5_m1_MD,df5_m1_md5[["MarkDown5","differ"]],on="differ",how="inner")
df5_m1_MD
Day	Month	Year	Store	Dept	Type	Weekly_Sales	Size	IsHoliday	Temperature	Fuel_Price	CPI	Unemployment	differ	MarkDown1	MarkDown2	MarkDown3	MarkDown4	MarkDown5
0	5	2	2010	1	1	0	24924.50	151315	0	42.31	2.572	211.096358	8.106	522010-1-1	6707.7404	7098.0724	148.4776	15135.2254	4855.9997
1	5	2	2010	1	2	0	50605.27	151315	0	42.31	2.572	211.096358	8.106	522010-1-2	6707.7404	7098.0724	148.4776	15135.2254	4855.9997
2	5	2	2010	1	3	0	13740.12	151315	0	42.31	2.572	211.096358	8.106	522010-1-3	6707.7404	7098.0724	148.4776	15135.2254	4855.9997
3	5	2	2010	1	4	0	39954.04	151315	0	42.31	2.572	211.096358	8.106	522010-1-4	6707.7404	7098.0724	148.4776	15135.2254	4855.9997
4	5	2	2010	1	5	0	32229.38	151315	0	42.31	2.572	211.096358	8.106	522010-1-5	6707.7404	7098.0724	148.4776	15135.2254	4855.9997
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
551474	26	7	2013	45	94	1	NaN	118221	0	76.06	3.804	NaN	NaN	2672013-45-94	212.0200	851.7300	2.0600	10.8800	1864.5700
551475	26	7	2013	45	95	1	NaN	118221	0	76.06	3.804	NaN	NaN	2672013-45-95	212.0200	851.7300	2.0600	10.8800	1864.5700
551476	26	7	2013	45	96	1	NaN	118221	0	76.06	3.804	NaN	NaN	2672013-45-96	212.0200	851.7300	2.0600	10.8800	1864.5700
551477	26	7	2013	45	97	1	NaN	118221	0	76.06	3.804	NaN	NaN	2672013-45-97	212.0200	851.7300	2.0600	10.8800	1864.5700
551478	26	7	2013	45	98	1	NaN	118221	0	76.06	3.804	NaN	NaN	2672013-45-98	212.0200	851.7300	2.0600	10.8800	1864.5700
551479 rows × 19 columns

df5_m1_cpi=RandomForest(df5_m1_MD,["Weekly_Sales","Unemployment"],"CPI")
{'Mean_squared_error': 1.3000141431668995e-16, 'Root_mean_squared_error': 1.140181627271243e-08, 'Mean_absolute_error': 1.8861340393900867e-10, 'R2_score': 1.0}
df5_m1_CPI=df5_m1_MD.drop(columns=["CPI"])
df5_m1_MDC=pd.merge(df5_m1_CPI,df5_m1_cpi[["CPI","differ"]],on="differ",how="inner")
df5_m1_unemp=RandomForest(df5_m1_MDC,["Weekly_Sales"],"Unemployment")
{'Mean_squared_error': 8.70435070945099e-28, 'Root_mean_squared_error': 2.950313662892641e-14, 'Mean_absolute_error': 2.1321529732500653e-14, 'R2_score': 1.0}
df5_m1_UNEMP=df5_m1_MDC.drop(columns=["Unemployment"])
df5_m1_MDCU=pd.merge(df5_m1_UNEMP,df5_m1_unemp[["Unemployment","differ"]],on="differ",how="inner")
df5_m1_ws=RandomForest(df5_m1_MDCU,[],"Weekly_Sales")
{'Mean_squared_error': 13109343.414650738, 'Root_mean_squared_error': 3620.682727698015, 'Mean_absolute_error': 1429.4126066382687, 'R2_score': 0.9736794460394441}
df5_m1_MDCUW=df5_m1_ws.copy()
#creating the customers column using of "Weekly_Sales" and "CPI"
df5_m1_MDCUW["Customers"]=df5_m1_MDCUW["Weekly_Sales"]/df5_m1_MDCUW["CPI"]
df5_m1_MDCUWC=df5_m1_MDCUW.copy()
df5_m1_MDCUWC.to_csv("model1_df.csv",index=False)
df_m1=pd.read_csv("C:/Users/visithra/Desktop/New folder/model1_df.csv")
df_M1=pd.merge(df5_m1_MDCU,df_m1[["Customers","differ"]],on="differ",how="inner")
df_M1.isnull().sum()
df_M1.to_csv("Model_1.csv",index=False)
#now we have a proper dataframe ,so ready to the meachine learning
#correlation checking for this dataframe

#normal dataframe
df_M1_corr=df_M1.drop(columns="differ").corr()
plt.figure(figsize=(15,8))
sns.heatmap(df_M1_corr,annot=True,cmap="Blues",fmt=".2f")
plt.show()

#"Weekly_Salse" have high correlation with the "Type", "Size", "Dept"
# Other wise have a low relation with the "Weekly_Salse"
df_M1= pd.read_csv("C:/Users/visithra/Desktop/New folder/Model_1.csv")
#separate the null columns for lable
df_M1_null=df_M1[df_M1["Weekly_Sales"].isnull()]
df_M1_null.reset_index(drop=True,inplace=True)

#separate the notnull columns for lable
df_M1_notnull= df_M1[df_M1["Weekly_Sales"].notnull()]
df_M1_notnull.reset_index(drop=True,inplace=True)

# chooshig the x and y value for the ML using notnull dataframe
x=df_M1_notnull.drop(columns=["Weekly_Sales","differ"])
y=df_M1_notnull["Weekly_Sales"]

#train_test_spliting
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)

model1=RandomForestRegressor().fit(x_train,y_train)
y_pred= model1.predict(x_test)

#evaluatet the model
mse=mean_squared_error(y_pred,y_test)
rmse= np.sqrt(mse)
mae=mean_absolute_error(y_pred,y_test)
r2= r2_score(y_pred,y_test)

metrics={"Mean_squared_error":mse,
            "Root_mean_squared_error":rmse,
            "Mean_absolute_error":mae,
            "R2_score":r2}

print(metrics)
{'Mean_squared_error': 119295.48853689854, 'Root_mean_squared_error': 345.391789909515, 'Mean_absolute_error': 22.997237668812403, 'R2_score': 0.9997645325315556}
y_pred1 = model1.predict(np.array([[5,2,2010,1,1,0,151315,0,42.31,2.572,6707.7404,7098.0724,148.4776,15135.2254,4855.9997,211.096358,8.106,118.071672]]))
y_pred1[0]
24934.6056
y_pred1 = model1.predict(np.array([[5,2,2010,1,1,1,151315,0,42.31,2.6,7046.9,6618.9,166.9,16055.8,4671.9,211.1,8.1,116.446049]]))
y_pred1[0]
24589.541400000006
with open(r"model_1_ML.pkl","wb") as a:
    pickle.dump(model1,a)
---------------------------------------------------------------------------
MemoryError                               Traceback (most recent call last)
c:\Users\visithra\Desktop\New folder\FinalProject2.ipynb Cell 95 line 2
      <a href='vscode-notebook-cell:/c%3A/Users/visithra/Desktop/New%20folder/FinalProject2.ipynb#Y161sZmlsZQ%3D%3D?line=0'>1</a> with open("model_1_ML.pkl","wb") as a:
----> <a href='vscode-notebook-cell:/c%3A/Users/visithra/Desktop/New%20folder/FinalProject2.ipynb#Y161sZmlsZQ%3D%3D?line=1'>2</a>     pickle.dump(model1,a)

MemoryError: 
WITHOUT MARKDOWN COLUMNS--->Model_2
df5_m2=df5.copy()
df5_m2=df5_m2.drop(columns="Markdown")
df5_m2
Day	Month	Year	Store	Dept	Type	Weekly_Sales	Size	IsHoliday	Temperature	Fuel_Price	MarkDown1	MarkDown2	MarkDown3	MarkDown4	MarkDown5	CPI	Unemployment	differ
0	5	2	2010	1	1	0	24924.50	151315	0	42.31	2.572	NaN	NaN	NaN	NaN	NaN	211.096358	8.106	522010-1-1
1	5	2	2010	1	2	0	50605.27	151315	0	42.31	2.572	NaN	NaN	NaN	NaN	NaN	211.096358	8.106	522010-1-2
2	5	2	2010	1	3	0	13740.12	151315	0	42.31	2.572	NaN	NaN	NaN	NaN	NaN	211.096358	8.106	522010-1-3
3	5	2	2010	1	4	0	39954.04	151315	0	42.31	2.572	NaN	NaN	NaN	NaN	NaN	211.096358	8.106	522010-1-4
4	5	2	2010	1	5	0	32229.38	151315	0	42.31	2.572	NaN	NaN	NaN	NaN	NaN	211.096358	8.106	522010-1-5
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
551474	26	7	2013	45	94	1	NaN	118221	0	76.06	3.804	212.02	851.73	2.06	10.88	1864.57	NaN	NaN	2672013-45-94
551475	26	7	2013	45	95	1	NaN	118221	0	76.06	3.804	212.02	851.73	2.06	10.88	1864.57	NaN	NaN	2672013-45-95
551476	26	7	2013	45	96	1	NaN	118221	0	76.06	3.804	212.02	851.73	2.06	10.88	1864.57	NaN	NaN	2672013-45-96
551477	26	7	2013	45	97	1	NaN	118221	0	76.06	3.804	212.02	851.73	2.06	10.88	1864.57	NaN	NaN	2672013-45-97
551478	26	7	2013	45	98	1	NaN	118221	0	76.06	3.804	212.02	851.73	2.06	10.88	1864.57	NaN	NaN	2672013-45-98
551479 rows × 19 columns

df5_m2=df5_m2.drop(columns=["MarkDown1","MarkDown2","MarkDown3","MarkDown4","MarkDown5",])
df5_m2
Day	Month	Year	Store	Dept	Type	Weekly_Sales	Size	IsHoliday	Temperature	Fuel_Price	CPI	Unemployment	differ
0	5	2	2010	1	1	0	24924.50	151315	0	42.31	2.572	211.096358	8.106	522010-1-1
1	5	2	2010	1	2	0	50605.27	151315	0	42.31	2.572	211.096358	8.106	522010-1-2
2	5	2	2010	1	3	0	13740.12	151315	0	42.31	2.572	211.096358	8.106	522010-1-3
3	5	2	2010	1	4	0	39954.04	151315	0	42.31	2.572	211.096358	8.106	522010-1-4
4	5	2	2010	1	5	0	32229.38	151315	0	42.31	2.572	211.096358	8.106	522010-1-5
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
551474	26	7	2013	45	94	1	NaN	118221	0	76.06	3.804	NaN	NaN	2672013-45-94
551475	26	7	2013	45	95	1	NaN	118221	0	76.06	3.804	NaN	NaN	2672013-45-95
551476	26	7	2013	45	96	1	NaN	118221	0	76.06	3.804	NaN	NaN	2672013-45-96
551477	26	7	2013	45	97	1	NaN	118221	0	76.06	3.804	NaN	NaN	2672013-45-97
551478	26	7	2013	45	98	1	NaN	118221	0	76.06	3.804	NaN	NaN	2672013-45-98
551479 rows × 14 columns

# ML function for RandomForestRegresser
def RandomForest(df,null_columns,lable):
    #droping the null columns
    df1=df.drop(columns=null_columns)

    #separate the null columns for lable
    df1_null=df1[df1[lable].isnull()]
    df1_null.reset_index(drop=True,inplace=True)

    #separate the notnull columns for lable
    df1_notnull= df1[df1[lable].notnull()]
    df1_notnull.reset_index(drop=True,inplace=True)

    # chooshig the x and y value for the ML using notnull dataframe
    x=df1_notnull.drop(columns=[lable,"differ"])
    y=df1_notnull[lable]

    #train_test_spliting
    x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)

    model=RandomForestRegressor().fit(x_train,y_train)
    y_pred= model.predict(x_test)

    #evaluatet the model
    mse=mean_squared_error(y_pred,y_test)
    rmse= np.sqrt(mse)
    mae=mean_absolute_error(y_pred,y_test)
    r2= r2_score(y_pred,y_test)

    metrics={"Mean_squared_error":mse,
             "Root_mean_squared_error":rmse,
             "Mean_absolute_error":mae,
             "R2_score":r2}
    
    print(metrics)
    
    #predict the null values for the df1_null dataframe
    test_data=df1_null.drop(columns=[lable,"differ"],axis=1)
    y_pred_null_df=model.predict(test_data)

    df1_null[lable]= pd.DataFrame(y_pred_null_df)

    df_result= pd.concat([df1_null,df1_notnull], axis=0, ignore_index=True)

    return df_result
#CPI prediction without markdown
df_m2_cpi=RandomForest(df5_m2,["Weekly_Sales","Unemployment"],"CPI")
{'Mean_squared_error': 5.666967017688136e-26, 'Root_mean_squared_error': 2.380539228344733e-13, 'Mean_absolute_error': 2.038620626816634e-13, 'R2_score': 1.0}
df_m2_CPI=df5_m2.drop(columns="CPI")
df_m2_C=pd.merge(df_m2_CPI,df_m2_cpi[["CPI","differ"]],on="differ",how="inner")
df_m2_unemp=RandomForest(df_m2_C,["Weekly_Sales"],"Unemployment")
{'Mean_squared_error': 8.903059253734247e-28, 'Root_mean_squared_error': 2.9837994660724515e-14, 'Mean_absolute_error': 2.1508640915874743e-14, 'R2_score': 1.0}
df_m2_UNEMP=df_m2_C.drop(columns="Unemployment")
df_m2_CU=pd.merge(df_m2_UNEMP,df_m2_unemp[["Unemployment","differ"]],on="differ",how="inner")
df_m2_ws=RandomForest(df_m2_CU,[],"Weekly_Sales")
{'Mean_squared_error': 11809702.563384484, 'Root_mean_squared_error': 3436.5247799753292, 'Mean_absolute_error': 1358.6930102435658, 'R2_score': 0.9762250068566835}
df_m2_CUW=df_m2_ws.copy()
#Creating the customers column using with the "Weekly_Sales" and the "CPI"
df_m2_CUW["Customers"]=df_m2_CUW["Weekly_Sales"]/df_m2_CUW["CPI"]
df_m2_CUW.to_csv("model2.csv",index=False)
df_m2_CU.isnull().sum()
Day                  0
Month                0
Year                 0
Store                0
Dept                 0
Type                 0
Weekly_Sales    131267
Size                 0
IsHoliday            0
Temperature          0
Fuel_Price           0
differ               0
CPI                  0
Unemployment         0
dtype: int64
df_M2=pd.merge(df_m2_CU,df_m2_CUW[["Customers","differ"]],on="differ",how="inner")
df_M2.to_csv("Model_2.csv",index=False)
#separate the null columns for lable
df_M2_null=df_M2[df_M2["Weekly_Sales"].isnull()]
df_M2_null.reset_index(drop=True,inplace=True)

#separate the notnull columns for lable
df_M2_notnull= df_M2[df_M2["Weekly_Sales"].notnull()]
df_M2_notnull.reset_index(drop=True,inplace=True)

# chooshig the x and y value for the ML using notnull dataframe
x=df_M2_notnull.drop(columns=["Weekly_Sales","differ"])
y=df_M2_notnull["Weekly_Sales"]

#train_test_spliting
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)

model2=RandomForestRegressor().fit(x_train,y_train)
y_pred= model2.predict(x_test)

#evaluatet the model
mse=mean_squared_error(y_pred,y_test)
rmse= np.sqrt(mse)
mae=mean_absolute_error(y_pred,y_test)
r2= r2_score(y_pred,y_test)

metrics={"Mean_squared_error":mse,
            "Root_mean_squared_error":rmse,
            "Mean_absolute_error":mae,
            "R2_score":r2}
with open(r"model_2.pkl","wb") as b:
    pickle.dump(model2,b)
with open(r"model_1_ML.pkl","rb") as a:
    pred_model_1=pickle.load(a)
with open(r"model_2.pkl","rb") as b:
    pred_model_2=pickle.load(b)
with open(r"C:/Users/visithra/Desktop/New folder/model2.pkl","rb") as rb:
    prediction_rb=pickle.load(rb)
---------------------------------------------------------------------------
MemoryError                               Traceback (most recent call last)
c:\Users\visithra\Desktop\New folder\FinalProject2.ipynb Cell 117 line 2
      <a href='vscode-notebook-cell:/c%3A/Users/visithra/Desktop/New%20folder/FinalProject2.ipynb#Y331sZmlsZQ%3D%3D?line=0'>1</a> with open(r"C:/Users/visithra/Desktop/New folder/model2.pkl","rb") as rb:
----> <a href='vscode-notebook-cell:/c%3A/Users/visithra/Desktop/New%20folder/FinalProject2.ipynb#Y331sZmlsZQ%3D%3D?line=1'>2</a>     prediction_rb=pickle.load(rb)

MemoryError: 
#prediction for any other values in model_1

y_pred1 = pred_model_1.predict(np.array([[5,2,2010,1,1,1,151315,0,42.31,2.6,7046.9,6618.9,166.9,16055.8,4671.9,211.1,8.1]]))
y_pred1[0]
#prediction for any other values in model2

y_pred1 = pred_model_2.predict(np.array([[5,2,2010,1,1,1,151315,0,42.31,2.6,211.1,8.1]]))
y_pred1[0]
Model the effects of markdowns on holiday weeks (with all markdown columns)
#model for the holiday weeks markdowns
df_m3= pd.read_csv("C:/Users/visithra/Desktop/New folder/model1_df.csv")
df_H= df_m3[df_m3["IsHoliday"]==1]
df_H.reset_index(drop=True, inplace= True)
df_NH= df_m3[df_m3["IsHoliday"]==0]
df_NH.reset_index(drop=True, inplace= True)
df_m3.groupby("IsHoliday")["MarkDown1"].mean()
IsHoliday
0     6747.936838
1    11096.379766
Name: MarkDown1, dtype: float64
df_m3.groupby("IsHoliday")["MarkDown2"].mean()
IsHoliday
0    2519.577466
1    9670.541788
Name: MarkDown2, dtype: float64
df_m3.groupby("IsHoliday")["MarkDown3"].mean()
IsHoliday
0      199.979369
1    14811.281163
Name: MarkDown3, dtype: float64
df_m3.groupby("IsHoliday")["MarkDown4"].mean()
IsHoliday
0    2970.740757
1    6557.897073
Name: MarkDown4, dtype: float64
df_m3.groupby("IsHoliday")["MarkDown5"].mean()
IsHoliday
0    4894.231155
1    3771.184706
Name: MarkDown5, dtype: float64
df_H
Day	Month	Year	Store	Dept	Type	Weekly_Sales	Size	IsHoliday	Temperature	Fuel_Price	differ	MarkDown1	MarkDown2	MarkDown3	MarkDown4	MarkDown5	CPI	Unemployment	Customers
0	12	2	2010	6	36	0	897.4775	202505	1	40.57	2.548	1222010-6-36	11341.7602	5182.2173	278.6200	18521.4332	7279.8994	212.770042	7.259	4.218063
1	12	2	2010	15	80	1	28.6648	123737	1	22.00	2.940	1222010-15-80	6302.8842	13302.6193	10.5367	5070.1600	5199.3263	131.586613	8.350	0.217840
2	12	2	2010	17	78	1	487.9827	93188	1	18.36	2.671	1222010-17-78	6300.8796	13232.3129	7.2552	3415.2387	2169.7566	126.496258	6.548	3.857685
3	12	2	2010	25	47	1	80.7168	128107	1	19.64	2.773	1222010-25-47	6302.8842	13302.6193	12.7968	4805.5351	4975.4224	204.385747	8.187	0.394924
4	12	2	2010	35	80	1	182.7054	103681	1	29.81	2.773	1222010-35-80	7064.9186	5817.8693	55.1945	19847.9630	7100.9583	135.411308	9.262	1.349263
...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...	...
39649	7	9	2012	45	93	1	3607.3700	118221	1	75.70	3.911	792012-45-93	11024.4500	12.8000	52.6300	1854.7700	2055.7000	191.577676	8.684	18.829804
39650	7	9	2012	45	94	1	3938.6300	118221	1	75.70	3.911	792012-45-94	11024.4500	12.8000	52.6300	1854.7700	2055.7000	191.577676	8.684	20.558919
39651	7	9	2012	45	95	1	52417.4700	118221	1	75.70	3.911	792012-45-95	11024.4500	12.8000	52.6300	1854.7700	2055.7000	191.577676	8.684	273.609489
39652	7	9	2012	45	97	1	7426.1900	118221	1	75.70	3.911	792012-45-97	11024.4500	12.8000	52.6300	1854.7700	2055.7000	191.577676	8.684	38.763337
39653	7	9	2012	45	98	1	352.4400	118221	1	75.70	3.911	792012-45-98	11024.4500	12.8000	52.6300	1854.7700	2055.7000	191.577676	8.684	1.839672
39654 rows × 20 columns

# chooshing the RandomForestRegressor
#Machine Learning for the Markdown
def Markdownprediction(df, lable):
    #separate the x and y for the train_test_split
    x= df.drop(columns=[lable,"differ"])
    y= df[lable]

    # train_test_split
    x_train,x_test,y_train,y_test= train_test_split(x,y,test_size=0.2,random_state=42)

    model= RandomForestRegressor().fit(x_train,y_train)
    y_pred= model.predict(x_test)

    #evaluate the model prediction
    mse= mean_squared_error(y_pred,y_test)
    rmse= np.sqrt(mse)
    mae= mean_absolute_error(y_pred,y_test)
    r2= r2_score(y_pred,y_test)

    metrics={"Mean_Squared_Error":mse,
            "Root_Mean_Squared_Error":rmse,
            "Mean_Absolute_Error":mae,
            "R2_Score":r2}
    
    print(metrics)
    
    if lable == "MarkDown1":
        with open(r"markdown_1.pkl","wb") as m1:
            pickle.dump(model,m1)

    elif lable == "MarkDown2":
        with open(r"markdown_2.pkl","wb") as m2:
            pickle.dump(model,m2)

    elif lable == "MarkDown3":
        with open(r"markdown_3.pkl","wb") as m3:
            pickle.dump(model,m3)
            
    elif lable == "MarkDown4":
        with open(r"markdown_4.pkl","wb") as m4:
            pickle.dump(model,m4)

    elif lable == "MarkDown5":
        with open(r"markdown_5.pkl","wb") as m5:
            pickle.dump(model,m5)
# model for the markdown1
df_H_M1= Markdownprediction(df_H,"MarkDown1")
{'Mean_Squared_Error': 9.51120451174205e-22, 'Root_Mean_Squared_Error': 3.0840240776852e-11, 'Mean_Absolute_Error': 1.4486233924683714e-11, 'R2_Score': 1.0}
df_H_M2= Markdownprediction(df_H,"MarkDown2")
{'Mean_Squared_Error': 7.09869556505009e-22, 'Root_Mean_Squared_Error': 2.664337734794538e-11, 'Mean_Absolute_Error': 1.1054406080603741e-11, 'R2_Score': 1.0}
df_H_M3= Markdownprediction(df_H,"MarkDown3")
{'Mean_Squared_Error': 1.9827844632222623e-21, 'Root_Mean_Squared_Error': 4.452846800892955e-11, 'Mean_Absolute_Error': 1.7233488638292713e-11, 'R2_Score': 1.0}
df_H_M4= Markdownprediction(df_H,"MarkDown4")
{'Mean_Squared_Error': 0.010081744099234981, 'Root_Mean_Squared_Error': 0.10040788863050044, 'Mean_Absolute_Error': 0.008048560971123128, 'R2_Score': 0.9999999999149235}
df_H_M5= Markdownprediction(df_H,"MarkDown5")
{'Mean_Squared_Error': 4.810529334254009e-23, 'Root_Mean_Squared_Error': 6.935797960043249e-12, 'Mean_Absolute_Error': 4.5125045212688926e-12, 'R2_Score': 1.0}
with open(r"C:/Users/visithra/Desktop/New folder/markdown_1.pkl","rb") as rm1:
    prediction_m1=pickle.load(rm1)
y_pred1 = prediction_m1.predict(np.array([[12,2,2010,6,36,0,897.4775,202505,1,40.57,2.548,5182.2173,278.62,18521.4332,7279.8994,212.770042,7.259,4.218063]]))
y_pred1
array([11341.7602])
with open(r"C:/Users/visithra/Desktop/New folder/markdown_2.pkl","rb") as rm2:
    prediction_m2=pickle.load(rm2)
y_pred2 = prediction_m2.predict(np.array([[12,2,2010,6,36,0,897.4775,202505,1,40.57,2.548,11341.7602,278.62,18521.4332,7279.8994,212.770042,7.259,4.218063]]))
y_pred2[0]
5182.217300000007
with open(r"C:/Users/visithra/Desktop/New folder/markdown_3.pkl","rb") as rm3:
    prediction_m3=pickle.load(rm3)
y_pred3 = prediction_m3.predict(np.array([[12,2,2010,6,36,0,897.4775,202505,1,40.57,2.548,11341.7602,5182.2173,18521.4332,7279.8994,212.770042,7.259,4.218063]]))
y_pred3[0]
278.6199999999993
with open(r"C:/Users/visithra/Desktop/New folder/markdown_4.pkl","rb") as rm4:
    prediction_m4=pickle.load(rm4)
y_pred4 = prediction_m4.predict(np.array([[12,2,2010,6,36,0,897.4775,202505,1,40.57,2.548,11341.7602,5182.2173,278.62,7279.8994,212.770042,7.259,4.218063]]))
y_pred4[0]
18521.433200000036
with open(r"C:/Users/visithra/Desktop/New folder/markdown_5.pkl","rb") as rm5:
    prediction_m5=pickle.load(rm5)
df_H.head(1)
Day	Month	Year	Store	Dept	Type	Weekly_Sales	Size	IsHoliday	Temperature	Fuel_Price	differ	MarkDown1	MarkDown2	MarkDown3	MarkDown4	MarkDown5	CPI	Unemployment	Customers
0	12	2	2010	6	36	0	897.4775	202505	1	40.57	2.548	1222010-6-36	11341.7602	5182.2173	278.62	18521.4332	7279.8994	212.770042	7.259	4.218063
y_pred5 = prediction_m5.predict(np.array([[12,2,2010,6,36,0,897.4775,202505,1,40.57,2.548,11341.7602,5182.2173,278.62,18521.4332,212.770042,7.259,4.218063]]))
y_pred5[0]
7279.899399999996
